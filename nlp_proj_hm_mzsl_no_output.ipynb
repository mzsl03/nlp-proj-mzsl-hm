{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import html\n",
        "import os\n",
        "import contractions\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.util import ngrams\n",
        "import unittest\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "import gradio as gr\n",
        "import random\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.processors import BertProcessing\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import (\n",
        "    Embedding, LSTM, GRU, Dense, Dropout,\n",
        "    Bidirectional, Conv1D, MaxPooling1D, Flatten,\n",
        "    Input, Lambda, Layer\n",
        ")\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "JOPourVPcjRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "id": "zHdbeq33dLju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"hf://datasets/elricwan/HarryPotter/data/train-00000-of-00001.parquet\")"
      ],
      "metadata": {
        "id": "WN5B8EwtZp_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hp_text = df['content']\n",
        "hp_text"
      ],
      "metadata": {
        "id": "mOniCrahaN4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hp_text = ' '.join(hp_text.astype(str).tolist())"
      ],
      "metadata": {
        "id": "mcp7J1btfR5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hp_text"
      ],
      "metadata": {
        "id": "SWjAjoTagJxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(hp_text)))\n",
        "print(f\"Unique characters: {len(chars)}\")"
      ],
      "metadata": {
        "id": "EkLq_IFyg-Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_counts = Counter(hp_text)\n",
        "top_chars = char_counts.most_common(20)"
      ],
      "metadata": {
        "id": "pospRTA2hF9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "chars_top = [c[0] for c in top_chars]\n",
        "counts_top = [c[1] for c in top_chars]\n",
        "axes[0, 0].bar(chars_top, counts_top, color='steelblue')\n",
        "axes[0, 0].set_title('Top 20 Character Frequencies', fontsize=14)\n",
        "axes[0, 0].set_xlabel('Character')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "words = re.findall(r'\\b\\w+\\b', hp_text[:100000])\n",
        "word_lengths = [len(w) for w in words]\n",
        "axes[0, 1].hist(word_lengths, bins=20, color='coral', edgecolor='black')\n",
        "axes[0, 1].set_title('Word Length Distribution', fontsize=14)\n",
        "axes[0, 1].set_xlabel('Word Length')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "\n",
        "axes[1, 0].axis('off')\n",
        "axes[1, 1].axis('off')\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
        "                      max_words=200, contour_width=3, contour_color='steelblue')\n",
        "wordcloud.generate(' '.join(words))\n",
        "axes[1, 1].imshow(wordcloud, interpolation='bilinear')\n",
        "axes[1, 1].set_title('Harry Potter Word Cloud', fontsize=14)"
      ],
      "metadata": {
        "id": "cejnuZ7khiPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"N-gram Analysis:\")\n",
        "bigrams = list(ngrams(words[:1000], 2))\n",
        "trigrams = list(ngrams(words[:1000], 3))\n",
        "\n",
        "bigram_counts = Counter(bigrams)\n",
        "trigram_counts = Counter(trigrams)\n",
        "\n",
        "print(f\"Most common bigrams: {bigram_counts.most_common(5)}\")\n",
        "print(f\"Most common trigrams: {trigram_counts.most_common(5)}\")"
      ],
      "metadata": {
        "id": "WCdpPRaqhjVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(text: str) -> str:\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "SAotyKiUZv2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_whitespace(text: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()"
      ],
      "metadata": {
        "id": "Fjmjw-x6Z8ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html_tags(text: str) -> str:\n",
        "    return BeautifulSoup(text, \"html.parser\").get_text()"
      ],
      "metadata": {
        "id": "INOQhAo8aCgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(text: str) -> str:\n",
        "    return contractions.fix(text)"
      ],
      "metadata": {
        "id": "MS-N0_MxaFC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = expand_contractions(text)\n",
        "    text = to_lowercase(text)\n",
        "    text = remove_html_tags(text)\n",
        "    text = normalize_whitespace(text)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "TnivrUAr8Irt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestTextCleaner(unittest.TestCase):\n",
        "\n",
        "    def test_to_lowercase(self):\n",
        "        self.assertEqual(to_lowercase(\"HELLO\"), \"hello\")\n",
        "\n",
        "    def test_normalize_whitespace(self):\n",
        "        self.assertEqual(normalize_whitespace(\" a   b  \"), \"a b\")\n",
        "\n",
        "    def test_remove_html_tags(self):\n",
        "        self.assertEqual(remove_html_tags(\"<p>Hello</p>\"), \"Hello\")\n",
        "\n",
        "    def test_expand_contractions(self):\n",
        "        self.assertEqual(expand_contractions(\"can't\"), \"cannot\")"
      ],
      "metadata": {
        "id": "agevkEYAOF8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=[''], exit=False)"
      ],
      "metadata": {
        "id": "oudgEw8UO0vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = clean_text(hp_text)"
      ],
      "metadata": {
        "id": "5r-2AhgGxXPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7fa14b7"
      },
      "source": [
        "hp_word_count = len(hp_text.split())\n",
        "cleaned_word_count = len(cleaned_text.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6047bc95"
      },
      "source": [
        "word_counts_data = {\n",
        "    'Original Text': hp_word_count,\n",
        "    'Cleaned Text': cleaned_word_count\n",
        "}\n",
        "\n",
        "word_counts_df = pd.DataFrame(word_counts_data.items(), columns=['Text Type', 'Word Count'])\n",
        "print(word_counts_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b40c802"
      },
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Text Type', y='Word Count', data=word_counts_df, palette='viridis', hue='Text Type', legend=False)\n",
        "plt.title('Word Count Comparison: Original vs. Cleaned Text')\n",
        "plt.xlabel('Text Type')\n",
        "plt.ylabel('Word Count')\n",
        "plt.ylim(2200000, 2400000)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"hp_corpus.txt\"\n",
        "\n",
        "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(hp_text)\n",
        "\n",
        "files = [file_path]"
      ],
      "metadata": {
        "id": "GV67SSNwI7RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train(files, vocab_size=30000, min_frequency=2)"
      ],
      "metadata": {
        "id": "WI9tFiDeIPSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"tokenizer_model\", exist_ok=True)"
      ],
      "metadata": {
        "id": "DvzonOHXovGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_model(\"tokenizer_model\")"
      ],
      "metadata": {
        "id": "fSPx64chJWgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPEPreprocessor:\n",
        "    def __init__(self, tokenizer_path, seq_length=20):\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        self.tokenizer = ByteLevelBPETokenizer(\n",
        "            tokenizer_path + \"/vocab.json\",\n",
        "            tokenizer_path + \"/merges.txt\"\n",
        "        )\n",
        "\n",
        "        self.vocab_size = self.tokenizer.get_vocab_size()\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        return self.tokenizer.decode(token_ids, skip_special_tokens=False)\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        return self.tokenizer.encode(text).ids\n",
        "\n",
        "    def create_sequences(self, token_ids, step=1):\n",
        "        sequences = []\n",
        "        next_tokens = []\n",
        "\n",
        "        for i in range(0, len(token_ids) - self.seq_length, step):\n",
        "            sequences.append(token_ids[i:i+self.seq_length])\n",
        "            next_tokens.append(token_ids[i+self.seq_length])\n",
        "\n",
        "        return sequences, next_tokens\n",
        "\n",
        "    def vectorize(self, sequences, next_tokens):\n",
        "        X = np.zeros((len(sequences), self.seq_length), dtype=np.int32)\n",
        "        y = np.array(next_tokens, dtype=np.int32)\n",
        "\n",
        "        for i, seq in enumerate(sequences):\n",
        "            X[i] = seq\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def preprocess(self, text, validation_split=0.1):\n",
        "        token_ids = self.encode_text(text)\n",
        "\n",
        "        sequences, next_tokens = self.create_sequences(token_ids)\n",
        "        X, y = self.vectorize(sequences, next_tokens)\n",
        "\n",
        "        split_idx = int(len(X) * (1 - validation_split))\n",
        "        return (\n",
        "            X[:split_idx], X[split_idx:],\n",
        "            y[:split_idx], y[split_idx:]\n",
        "        )"
      ],
      "metadata": {
        "id": "g4tG-dzFIPmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPreprocessor:\n",
        "\n",
        "  def __init__(self, text, seq_length=100):\n",
        "        self.text = clean_text(text)\n",
        "        self.seq_length = seq_length\n",
        "        self.chars = sorted(list(set(text)))\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "  def create_sequences(self):\n",
        "        step = 3\n",
        "\n",
        "        sentences = []\n",
        "        next_chars = []\n",
        "\n",
        "        for i in tqdm(range(0, len(self.text) - self.seq_length, step)):\n",
        "            sentences.append(self.text[i:i + self.seq_length])\n",
        "            next_chars.append(self.text[i + self.seq_length])\n",
        "\n",
        "        return sentences, next_chars\n",
        "\n",
        "  def vectorize_sequences(self, sentences, next_chars):\n",
        "\n",
        "        X = np.zeros((len(sentences), self.seq_length, self.vocab_size), dtype=np.bool_)\n",
        "        y = np.zeros((len(sentences), self.vocab_size), dtype=np.bool_)\n",
        "\n",
        "        for i, sentence in tqdm(enumerate(sentences)):\n",
        "            for t, char in enumerate(sentence):\n",
        "                X[i, t, self.char_to_idx[char]] = 1\n",
        "            y[i, self.char_to_idx[next_chars[i]]] = 1\n",
        "\n",
        "        return X, y\n",
        "\n",
        "  def preprocess_for_rnn(self, validation_split=0.1):\n",
        "        sentences, next_chars = self.create_sequences()\n",
        "        X, y = self.vectorize_sequences(sentences, next_chars)\n",
        "\n",
        "        split_idx = int(len(X) * (1 - validation_split))\n",
        "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "        return X_train, X_val, y_train, y_val, sentences, next_chars"
      ],
      "metadata": {
        "id": "_trnbZFPkQOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordLevelPreprocessor:\n",
        "    def __init__(self, text, seq_length=20, min_word_freq=2):\n",
        "        self.text = clean_text(text)\n",
        "        self.seq_length = seq_length\n",
        "        self.min_word_freq = min_word_freq\n",
        "        self.words = word_tokenize(self.text)\n",
        "        self._build_vocabulary()\n",
        "\n",
        "    def _build_vocabulary(self):\n",
        "        word_counts = Counter(self.words)\n",
        "\n",
        "        self.vocab = [word for word, count in word_counts.items()\n",
        "                     if count >= self.min_word_freq]\n",
        "\n",
        "        self.vocab = ['<UNK>', '<PAD>', '<START>', '<END>'] + self.vocab\n",
        "\n",
        "        self.word_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
        "        self.idx_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "\n",
        "        self.words_processed = [\n",
        "            word if word in self.word_to_idx and word not in ['<UNK>', '<PAD>', '<START>', '<END>']\n",
        "            else '<UNK>'\n",
        "            for word in self.words\n",
        "        ]\n",
        "\n",
        "    def create_sequences(self, step=1):\n",
        "        sequences = []\n",
        "        next_words = []\n",
        "\n",
        "        for i in tqdm(range(0, len(self.words_processed) - self.seq_length, step)):\n",
        "            sequences.append(self.words_processed[i:i + self.seq_length])\n",
        "            next_words.append(self.words_processed[i + self.seq_length])\n",
        "\n",
        "        return sequences, next_words\n",
        "\n",
        "    def vectorize_sequences(self, sequences, next_words):\n",
        "        X = np.zeros((len(sequences), self.seq_length), dtype=np.int32)\n",
        "        y = np.zeros((len(sequences), self.vocab_size), dtype=np.bool_)\n",
        "\n",
        "        for i, seq in tqdm(enumerate(sequences)):\n",
        "            for t, word in enumerate(seq):\n",
        "                X[i, t] = self.word_to_idx.get(word, self.word_to_idx['<UNK>'])\n",
        "            y[i, self.word_to_idx.get(next_words[i], self.word_to_idx['<UNK>'])] = 1\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def preprocess_for_rnn(self, validation_split=0.1):\n",
        "        sequences, next_words = self.create_sequences(step=10)\n",
        "        X, y = self.vectorize_sequences(sequences, next_words)\n",
        "\n",
        "        split_idx = int(len(X) * (1 - validation_split))\n",
        "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "        return X_train, X_val, y_train, y_val\n",
        "\n",
        "    def words_to_indices(self, words):\n",
        "        return [self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in words]\n",
        "\n",
        "    def indices_to_words(self, indices):\n",
        "        return [self.idx_to_word[idx] for idx in indices]"
      ],
      "metadata": {
        "id": "uzwK7AtW3WQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_preprocessor = BPEPreprocessor(\"tokenizer_model\", seq_length=20)"
      ],
      "metadata": {
        "id": "09pL2ZfKJ6Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = TextPreprocessor(hp_text[:100000], seq_length=60)\n",
        "word_preprocessor = WordLevelPreprocessor(\n",
        "    text=hp_text[:100000],\n",
        "    seq_length=15,\n",
        "    min_word_freq=2\n",
        ")"
      ],
      "metadata": {
        "id": "RgF_4RDNkZ-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val, sentences, next_chars = preprocessor.preprocess_for_rnn(validation_split=0.1)"
      ],
      "metadata": {
        "id": "vssmLjzAk24U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(vocab_size, seq_length, lstm_units=128):\n",
        "    model = Sequential([\n",
        "        LSTM(lstm_units, input_shape=(seq_length, vocab_size), return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        LSTM(lstm_units),\n",
        "        Dropout(0.2),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "8SZMW8MJk7Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = build_lstm_model(preprocessor.vocab_size, preprocessor.seq_length)\n",
        "lstm_model.summary()"
      ],
      "metadata": {
        "id": "6lTxaRtJlVE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bidirectional_lstm(vocab_size, seq_length, lstm_units=128):\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(lstm_units, return_sequences=True),\n",
        "                     input_shape=(seq_length, vocab_size)),\n",
        "        Dropout(0.3),\n",
        "        Bidirectional(LSTM(lstm_units)),\n",
        "        Dropout(0.3),\n",
        "        Dense(lstm_units // 2, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "lTip6cwssi8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bi_lstm_model = build_bidirectional_lstm(preprocessor.vocab_size, preprocessor.seq_length)\n",
        "bi_lstm_model.summary()"
      ],
      "metadata": {
        "id": "Ggh-OaGbso8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "]"
      ],
      "metadata": {
        "id": "7j8S2qk1li-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bi_lstm_history = bi_lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=256,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "08eIsqIBsvLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_history = lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=256,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "3T9iJlQyl2a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gru_model(vocab_size, seq_length, gru_units=128):\n",
        "    model = Sequential([\n",
        "        GRU(gru_units, return_sequences=True, input_shape=(seq_length, vocab_size)),\n",
        "        Dropout(0.2),\n",
        "        GRU(gru_units),\n",
        "        Dropout(0.2),\n",
        "        Dense(gru_units // 2, activation='relu'),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "8r46Ed7yoJSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model = build_gru_model(preprocessor.vocab_size, preprocessor.seq_length)\n",
        "gru_model.summary()"
      ],
      "metadata": {
        "id": "TEEUXyNRoNJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_history = gru_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=256,\n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "5xBrbwRZoPPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = word_preprocessor.preprocess_for_rnn()"
      ],
      "metadata": {
        "id": "D5xiUh6747K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_word_level_model(vocab_size, seq_length, embedding_dim=256, lstm_units=256):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size,\n",
        "                 output_dim=embedding_dim,\n",
        "                 input_length=seq_length,\n",
        "                 mask_zero=True),\n",
        "        LSTM(lstm_units, return_sequences=True, use_cudnn=False),\n",
        "        Dropout(0.3),\n",
        "        LSTM(lstm_units // 2, use_cudnn=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(lstm_units // 2, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ioeJWi7B2Ofz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_level_model = build_word_level_model(\n",
        "    vocab_size=word_preprocessor.vocab_size,\n",
        "    seq_length=word_preprocessor.seq_length,\n",
        ")\n",
        "word_level_model.summary()"
      ],
      "metadata": {
        "id": "vPReNzEw2VmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_level_model_history = word_level_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=128,\n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "vJoW7IVY2lIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bpe_text_stable(model, preprocessor, seed_text, length=50, temperature=0.7, top_k=0, top_p=0.9, repetition_penalty=1.2):\n",
        "    seed_ids = preprocessor.encode_text(seed_text)\n",
        "\n",
        "    if len(seed_ids) > preprocessor.seq_length:\n",
        "        seed_ids = seed_ids[-preprocessor.seq_length:]\n",
        "    elif len(seed_ids) < preprocessor.seq_length:\n",
        "        pad_len = preprocessor.seq_length - len(seed_ids)\n",
        "        seed_ids = [0] * pad_len + seed_ids\n",
        "\n",
        "    generated_ids = seed_ids.copy()\n",
        "\n",
        "    for _ in range(length):\n",
        "        x_pred = np.array([seed_ids], dtype=np.int32)\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "\n",
        "        log_probs = np.log(np.maximum(preds, 1e-7))\n",
        "\n",
        "        seen_ids = [id for id in seed_ids if id != 0]\n",
        "\n",
        "        for token_id in seen_ids:\n",
        "            logit_i = log_probs[token_id]\n",
        "\n",
        "            if logit_i >= 0:\n",
        "                log_probs[token_id] = logit_i / repetition_penalty\n",
        "            else:\n",
        "                log_probs[token_id] = logit_i * repetition_penalty\n",
        "\n",
        "        log_probs_max = np.max(log_probs)\n",
        "        exp_preds = np.exp(log_probs - log_probs_max)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        preds /= np.sum(preds)\n",
        "\n",
        "        if top_p > 0 and top_p < 1:\n",
        "            sorted_indices = np.argsort(preds)[::-1]\n",
        "            sorted_preds = preds[sorted_indices]\n",
        "\n",
        "            cumulative_probs = np.cumsum(sorted_preds)\n",
        "\n",
        "            nucleus_indices = np.where(cumulative_probs >= top_p)[0]\n",
        "\n",
        "            if nucleus_indices.size == 0:\n",
        "                top_indices = sorted_indices[:1]\n",
        "            else:\n",
        "                nucleus_index = nucleus_indices[0] + 1\n",
        "                top_indices = sorted_indices[:nucleus_index]\n",
        "\n",
        "            top_probs = preds[top_indices]\n",
        "\n",
        "        elif top_k > 0:\n",
        "            top_indices = np.argsort(preds)[-top_k:]\n",
        "            top_probs = preds[top_indices]\n",
        "\n",
        "        else:\n",
        "            top_indices = np.arange(len(preds))\n",
        "            top_probs = preds\n",
        "\n",
        "        top_probs /= np.sum(top_probs)\n",
        "\n",
        "        next_id = np.random.choice(top_indices, p=top_probs)\n",
        "\n",
        "        generated_ids.append(next_id)\n",
        "        seed_ids = seed_ids[1:] + [next_id]\n",
        "\n",
        "    final_ids = [id for id in generated_ids if id != 0]\n",
        "\n",
        "    return preprocessor.decode(final_ids)"
      ],
      "metadata": {
        "id": "jBkosqLrTCkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "jUDsseBhvgs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, num_chars=300, temperature=1.0):\n",
        "    generated = seed_text\n",
        "\n",
        "    for i in range(num_chars):\n",
        "        x_pred = np.zeros((1, preprocessor.seq_length, preprocessor.vocab_size))\n",
        "        for t, char in enumerate(seed_text):\n",
        "            if char in preprocessor.char_to_idx:\n",
        "                x_pred[0, t, preprocessor.char_to_idx[char]] = 1.\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, temperature)\n",
        "        next_char = preprocessor.idx_to_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "yiPO8wlAvpyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models_generation(seed_text, num_chars=200, temperature=0.7):\n",
        "    print(f\"COMPARING MODELS WITH SEED: '{seed_text}'\")\n",
        "    print(f\"Temperature: {temperature}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    models = {\n",
        "        \"LSTM\": lstm_model,\n",
        "        \"Bidirectional LSTM\": bi_lstm_model,\n",
        "        \"GRU\": gru_model\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n{name}:\")\n",
        "        generated = generate_text(model, seed_text, num_chars, temperature)\n",
        "        print(generated)\n",
        "        results[name] = generated\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "hPxpsPtIvycY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seeds = [\n",
        "    \"harry potter was a very unusual boy\",\n",
        "    \"the dark lord shall rise again\",\n",
        "    \"hermione opened the ancient book and read\",\n",
        "    \"in the great hall of hogwarts, dumbledore\"\n",
        "]\n",
        "\n",
        "for seed in test_seeds[:2]:\n",
        "    compare_models_generation(seed, num_chars=150, temperature=0.8)\n"
      ],
      "metadata": {
        "id": "1fuwVI9rv3bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "KLm2iInzs8x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.dropout3 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, encoder_output=None):\n",
        "        batch_size, seq_length = ops.shape(x)[0], ops.shape(x)[1]\n",
        "        causal_mask = ops.triu(\n",
        "            ops.ones((batch_size, 1, seq_length, seq_length)) * -np.inf,\n",
        "            k=1\n",
        "        )\n",
        "\n",
        "        attn1 = self.mha1(x, x, attention_mask=causal_mask, use_causal_mask=True)\n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "\n",
        "        if encoder_output is not None:\n",
        "            attn2 = self.mha2(out1, encoder_output, encoder_output)\n",
        "            attn2 = self.dropout2(attn2)\n",
        "            out2 = self.layernorm2(out1 + attn2)\n",
        "        else:\n",
        "            out2 = self.layernorm2(out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        return self.layernorm3(out2 + ffn_output)"
      ],
      "metadata": {
        "id": "cxR6snVurWyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer_model(vocab_size, seq_length, num_layers, embed_dim=128, num_heads=8, ff_dim=512):\n",
        "    inputs = tf.keras.layers.Input(shape=(seq_length,))\n",
        "\n",
        "    embedding_layer = TokenAndPositionEmbedding(seq_length, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "      transformer_block = TransformerDecoder(embed_dim, num_heads, ff_dim)\n",
        "      x = transformer_block(x)\n",
        "\n",
        "    x = tf.keras.layers.Lambda(lambda x: x[:, -1, :])(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(embed_dim, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Z2YkrKzDtCoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_preprocessor = BPEPreprocessor(\"tokenizer_model\", seq_length=100)\n",
        "X_train_bpe, X_val_bpe, y_train_bpe, y_val_bpe = bpe_preprocessor.preprocess(\n",
        "    hp_text\n",
        ")"
      ],
      "metadata": {
        "id": "62YbfiTutLcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = build_transformer_model(\n",
        "    vocab_size=bpe_preprocessor.vocab_size,\n",
        "    seq_length=bpe_preprocessor.seq_length,\n",
        "    embed_dim=300,\n",
        "    num_heads=10,\n",
        "    ff_dim=800,\n",
        "    num_layers=2\n",
        ")\n",
        "transformer_model.summary()"
      ],
      "metadata": {
        "id": "wGQeYseKtPD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, min_delta=0.02),\n",
        "    ]\n",
        "\n",
        "transformer_history = transformer_model.fit(\n",
        "    X_train_bpe, y_train_bpe,\n",
        "    validation_data=(X_val_bpe, y_val_bpe),\n",
        "    batch_size=1024,\n",
        "    epochs=5,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "pbIJA9XAtQ4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_bpe = generate_bpe_text_stable(\n",
        "      model=transformer_model,\n",
        "      preprocessor=bpe_preprocessor,\n",
        "      seed_text=clean_text(\"\"\"Harry Potter, Hermione Granger and Ron Weasley stood silently in the Great Hall,\n",
        "      listening to Professor Dumbledore's words about the return of the Dark Lord.\n",
        "      \"\"\"),\n",
        "      length=150,\n",
        "      temperature=.75,\n",
        "      top_p=.8)\n",
        "\n",
        "\n",
        "generated_bpe"
      ],
      "metadata": {
        "id": "CnVtm4Dhz48g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e54c685"
      },
      "source": [
        "seed_texts = [\n",
        "    \"harry potter was a very unusual boy\",\n",
        "    \"the dark lord shall rise again\",\n",
        "    \"hermione opened the ancient book and read\",\n",
        "    \"in the great hall of hogwarts, dumbledore\",\n",
        "    \"draco malfoy smirked, his pale face reflecting the light\",\n",
        "    \"voldemort's name was rarely spoken aloud, for fear\",\n",
        "    \"the forbidden forest loomed dark and mysterious\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11baa220"
      },
      "source": [
        "def generate_text_gradio(seed_text, length, temperature, top_k, top_p):\n",
        "    if not seed_text or seed_text.strip() == \"\":\n",
        "        seed_text = random.choice(seed_texts)\n",
        "\n",
        "    cleaned_seed_text = clean_text(seed_text)\n",
        "\n",
        "    generated_text = generate_bpe_text_stable(\n",
        "        model=transformer_model,\n",
        "        preprocessor=bpe_preprocessor,\n",
        "        seed_text=cleaned_seed_text,\n",
        "        length=length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p\n",
        "    )\n",
        "    return generated_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa0de093"
      },
      "source": [
        "iface = gr.Interface(\n",
        "    fn=generate_text_gradio,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Seed Text (leave blank for random)\", lines=2),\n",
        "        gr.Slider(minimum=10, maximum=500, value=150, step=10, label=\"Generation Length\"),\n",
        "        gr.Slider(minimum=0.1, maximum=2.0, value=0.75, step=0.05, label=\"Temperature\"),\n",
        "        gr.Slider(minimum=0, maximum=100, value=0, step=1, label=\"Top K (0 for disabled)\"),\n",
        "        gr.Slider(minimum=0.0, maximum=1.0, value=0.8, step=0.05, label=\"Top P (0 for disabled)\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Text\", lines=10),\n",
        "    title=\"Harry Potter Text Generator (Transformer)\",\n",
        "    description=\"Generate Harry Potter-style text using a fine-tuned Transformer model. Leave seed text blank for a random starting phrase.\"\n",
        ")\n",
        "\n",
        "print(\"Gradio interface created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbaa2b8b"
      },
      "source": [
        "iface.launch(debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0b28e4f"
      },
      "source": [
        "def calculate_metrics(reference_text, generated_text):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = scorer.score(reference_text, generated_text)\n",
        "\n",
        "    reference_tokens = nltk.word_tokenize(reference_text)\n",
        "    generated_tokens = nltk.word_tokenize(generated_text)\n",
        "\n",
        "    bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
        "\n",
        "    meteor = meteor_score([reference_tokens], generated_tokens)\n",
        "\n",
        "    return {\n",
        "        \"rouge\": rouge_scores,\n",
        "        \"bleu\": bleu_score,\n",
        "        \"meteor\": meteor\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2936464a"
      },
      "source": [
        "reference_text = \"Harry Potter, Hermione Granger and Ron Weasley stood silently in the Great Hall, listening to Professor Dumbledore's words about the return of the Dark Lord. The atmosphere was heavy with dread, and the young wizards braced themselves for the coming battle.\"\n",
        "seed_text_for_eval = \"Harry Potter, Hermione Granger and Ron Weasley stood silently in the Great Hall, listening to Professor Dumbledore's words about the return of the Dark Lord.\"\n",
        "\n",
        "generated_text_for_eval = generate_bpe_text_stable(\n",
        "    model=transformer_model,\n",
        "    preprocessor=bpe_preprocessor,\n",
        "    seed_text=clean_text(seed_text_for_eval),\n",
        "    length=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.8\n",
        ")\n",
        "\n",
        "evaluation_results = calculate_metrics(reference_text, generated_text_for_eval)\n",
        "\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "print(f\"Reference: {reference_text}\")\n",
        "print(f\"Generated: {generated_text_for_eval}\")\n",
        "print(f\"ROUGE Scores: {evaluation_results['rouge']}\")\n",
        "print(f\"BLEU Score: {evaluation_results['bleu']}\")\n",
        "print(f\"METEOR Score: {evaluation_results['meteor']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2b0676f"
      },
      "source": [
        "rouge1 = evaluation_results['rouge']['rouge1']\n",
        "rouge2 = evaluation_results['rouge']['rouge2']\n",
        "rougeL = evaluation_results['rouge']['rougeL']\n",
        "\n",
        "bleu_score = evaluation_results['bleu']\n",
        "meteor_score = evaluation_results['meteor']\n",
        "\n",
        "rouge_labels = ['R1-P', 'R1-R', 'R1-F1', 'R2-P', 'R2-R', 'R2-F1', 'RL-P', 'RL-R', 'RL-F1']\n",
        "rouge_values = [\n",
        "    rouge1.precision, rouge1.recall, rouge1.fmeasure,\n",
        "    rouge2.precision, rouge2.recall, rouge2.fmeasure,\n",
        "    rougeL.precision, rougeL.recall, rougeL.fmeasure\n",
        "]\n",
        "\n",
        "other_metrics_labels = ['BLEU', 'METEOR']\n",
        "other_metrics_values = [bleu_score, meteor_score]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "axes[0].bar(rouge_labels, rouge_values, color=['skyblue', 'salmon', 'lightgreen']*3)\n",
        "axes[0].set_title('ROUGE Scores (Precision, Recall, F1)')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].set_ylim(0, 1)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "axes[1].bar(other_metrics_labels, other_metrics_values, color=['purple', 'orange'])\n",
        "axes[1].set_title('BLEU and METEOR Scores')\n",
        "axes[1].set_ylabel('Score')\n",
        "axes[1].set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7fdfd39"
      },
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "fig.suptitle('Training and Validation Loss for All Models', fontsize=16)\n",
        "\n",
        "axes[0, 0].plot(bi_lstm_history.history['loss'], label='Train Loss')\n",
        "axes[0, 0].plot(bi_lstm_history.history['val_loss'], label='Validation Loss')\n",
        "axes[0, 0].set_title('Bidirectional LSTM Learning Curve')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "axes[0, 1].plot(lstm_history.history['loss'], label='Train Loss')\n",
        "axes[0, 1].plot(lstm_history.history['val_loss'], label='Validation Loss')\n",
        "axes[0, 1].set_title('LSTM Learning Curve')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Loss')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "axes[1, 0].plot(gru_history.history['loss'], label='Train Loss')\n",
        "axes[1, 0].plot(gru_history.history['val_loss'], label='Validation Loss')\n",
        "axes[1, 0].set_title('GRU Learning Curve')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "axes[1, 1].plot(transformer_history.history['loss'], label='Train Loss')\n",
        "axes[1, 1].plot(transformer_history.history['val_loss'], label='Validation Loss')\n",
        "axes[1, 1].set_title('Transformer Learning Curve')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Loss')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Konklzi\n",
        "\n",
        "A karakteralap RNN-modellek (LSTM, BiLSTM, GRU) kpesek alapvet mintkat megtanulni, de korltozott kontextuskezelsk miatt a generlt szveg gyakran ismtld vagy kevsb koherens. A GRU stabilan tanult, a BiLSTM pedig javtott a kontextusrzkelsen, de tovbbra is karakterszinten maradtak. Ezzel szemben a Transformer modell sokkal termszetesebb, hosszabb tv sszefggseket is megtart s stlusosabb szveget lltott el. A Transformer bizonyult a legjobban teljest modellnek emberi olvasatra, amely nagy rszben a BPE tokenizcinak ksznhet. sszessgben ez a modell adta a legalkalmazhatbb s leglethbb eredmnyeket a generatv feladatban."
      ],
      "metadata": {
        "id": "LvfWamUknMVJ"
      }
    }
  ]
}